{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('dl')"
  },
  "interpreter": {
   "hash": "753cf74a1b63db9003c1ef07366732ff5db983517e22597d3b51fe3bb51ca165"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys, os\n",
    "\n",
    "# Add utility_scripts in the current path so that they can be imported directly just like in interactive mode\n",
    "sys.path.append(os.path.abspath(\"../usr/lib/\"))\n",
    "for script_folder in os.listdir(\"../usr/lib/\"):\n",
    "    sys.path.append(os.path.abspath(\"../usr/lib/\"+script_folder))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from datetime import date\n",
    "from statistics import mean, median\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from lastquerytransformer import Riiid\n",
    "from riiidutils import RiiidDataset, riiid_collate_fn, riiid_collate_fn_right_padding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "loc = os.environ.get('KAGGLE_KERNEL_RUN_TYPE','Localhost')\n",
    "if loc == 'Interactive' or loc == 'Localhost':\n",
    "    conf = {\n",
    "        'train_size': 96_000,\n",
    "        'batch_size': 128,\n",
    "        \"max_length\": 1728,\n",
    "        # model\n",
    "        \"dropout\": 0.1,\n",
    "        # training loop\n",
    "        'epochs': 50,\n",
    "        'eval_steps': 250,\n",
    "        # adam\n",
    "        'learning_rate': 1e-3,\n",
    "        'epsilon': 1e-8,\n",
    "        \"weight_decay\": 0.01,\n",
    "        # clip_grad_value\n",
    "        \"clip_value\": False\n",
    "    }\n",
    "# When it is run after an api push.\n",
    "elif loc == 'Batch':\n",
    "    conf = {\n",
    "        'train_size': 400_000,\n",
    "        'batch_size': 256,\n",
    "        \"max_length\": 1728,\n",
    "        # model\n",
    "        \"dropout\": 0.1,\n",
    "        # training loop\n",
    "        'epochs': 40,\n",
    "        'eval_steps': 500,\n",
    "        # adam\n",
    "        'learning_rate': 1e-3,\n",
    "        'epsilon': 1e-8,\n",
    "        \"weight_decay\": 0.01,\n",
    "        # clip_grad_value\n",
    "        \"clip_value\": False\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Le fichier train.csv comprend un peu plus de 100 millions de lignes.  \n",
    "Il y a un peu moins de 400 mille user_id uniques.  \n",
    "Le modèle utilisé dans ce notebook prend en entrée une série par utilisateur."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Features utilisées\n",
    "Pour chaque question de la série d'apprentissage d'un utilisateur quelconque  \n",
    "\n",
    "1. Question ID: correspond à content_id (lorsque l'élément est une question).\n",
    "2. Question part: correspond à part dans question.csv pour l'élément dont question_id correspondant à content_id\n",
    "3. Answer correctness: valeur de answered_correctly de l'exemple (ou target encoding de la question ?)\n",
    "4. Current question elapsed time: prior question de la question suivante.\n",
    "5. Timestamp difference: current question timestamp - timestamp of the last question from the same user"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chargement des données et instanciation des datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open('../input/riiid-sequences/users_y.pickle', 'rb') as f:\n",
    "    users_y = pickle.load(f)\n",
    "with open('../input/riiid-sequences/users_cat.pickle', 'rb') as f:\n",
    "    users_cat = pickle.load(f)\n",
    "with open('../input/riiid-sequences/users_cont.pickle', 'rb') as f:\n",
    "    users_cont = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "seed = 12\n",
    "cat_train, cat_val, cont_train, cont_val, y_train, y_val = train_test_split(users_cat, users_cont, users_y, test_size=.05, random_state=seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cat_train = cat_train[:conf['train_size']]\n",
    "cont_train = cont_train[:conf['train_size']]\n",
    "y_train = y_train[:conf['train_size']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(\"Number of train examples:\", len(y_train))\n",
    "print(\"Number of valid examples:\", len(y_val))\n",
    "print(\"Train set answered_correctly average value:\", \"{:.3}\".format(y_train.mean()))\n",
    "print(\"Valid set answered_correctly average value:\", \"{:.3}\".format(y_val.mean()))\n",
    "print(\"Train set median sequence length:\", \"{:.0f}\".format(median([user_seq.shape[0] for user_seq in cat_train])))\n",
    "print(\"Valid set median sequence length:\", \"{:.0f}\".format(median([user_seq.shape[0] for user_seq in cat_val])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train examples: 96000\n",
      "Number of valid examples: 19683\n",
      "Train set answered_correctly average value: 0.472\n",
      "Valid set answered_correctly average value: 0.475\n",
      "Train set median sequence length: 40\n",
      "Valid set median sequence length: 41\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batches\n",
    "Dynamic Padding: ajout de padding batch par batch pour avoir une même longueur de séquence dans chaque batch.  \n",
    "Uniform size batching: on trie les utilisateurs par longueur de séquence, afin d'avoir des longueurs plus proches dans chaque batch  \n",
    "\n",
    "Afin de mettre en œuvre ces deux stratégies on va par simplicité trier au préalable et conjointement les listes batch_cat, batch_cont et batch_y par longueur des séquences dans batch_cat/batch_cont (c'est RiiidDataset qui s'en charge). Le DataLoader utilisera une fonction collate_fn permettant d'ajouter du padding dynamiquement batch par batch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# train_dataset = RiiidDataset(cat_train, cont_train, y_train, sort_sequences=False)\n",
    "# val_dataset = RiiidDataset(cat_val, cont_val, y_val, sort_sequences=False)\n",
    "train_dataset = RiiidDataset(cat_train, cont_train, y_train, sort_sequences=True, max_length=conf[\"max_length\"])\n",
    "val_dataset = RiiidDataset(cat_val, cont_val, y_val, sort_sequences=True, max_length=conf[\"max_length\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=conf['batch_size'], shuffle=False, collate_fn=riiid_collate_fn, drop_last=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4*conf['batch_size'], shuffle=False, collate_fn=riiid_collate_fn, pin_memory=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=conf['batch_size'], shuffle=False, collate_fn=riiid_collate_fn_right_padding, drop_last=True, pin_memory=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4*conf['batch_size'], shuffle=False, collate_fn=riiid_collate_fn_right_padding, pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Without dynamic batching"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# cat_train = pad_sequence_left([torch.tensor(el) for el in cat_train], batch_first=True)\n",
    "# cont_train = pad_sequence_left([torch.tensor(el, dtype=torch.float) for el in cont_train], batch_first=True)\n",
    "# #y = torch.tensor(y, dtype=torch.float)\n",
    "# cat_val = pad_sequence_left([torch.tensor(el) for el in cat_val], batch_first=True)\n",
    "# cont_val = pad_sequence_left([torch.tensor(el, dtype=torch.float) for el in cont_val], batch_first=True)\n",
    "# #y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# train_dataset = RiiidDataset(cat_train, cont_train, y_train, sort_sequences=False)\n",
    "# val_dataset = RiiidDataset(cat_val, cont_val, y_val, sort_sequences=False)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=conf['batch_size'], shuffle=False, drop_last=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4*conf['batch_size'], shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modèle\n",
    "Pour le modèle, l'auteur s'est inspiré de la solution arrivée 3è à la compétition data Science bowl 2019 ([discussion](https://www.kaggle.com/c/data-science-bowl-2019/discussion/127891), [code](https://www.kaggle.com/limerobot/dsb2019-v77-tr-dt-aug0-5-3tta))  \n",
    "La procédure de création des embeddings est expliquée, on suppose que l'auteur s'est basé dessus.\n",
    "## Embeddings\n",
    "On utilise un embedding catégoriel pour les 3 premières variables et un embedding continu pour les 2 dernières."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "maximums = {'question_id': 13523, 'part': 7, 'answered_correctly': 3}\n",
    "model = Riiid(maximums, dropout=conf[\"dropout\"]).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=conf['learning_rate'], eps=conf['epsilon'], weight_decay=conf[\"weight_decay\"])\n",
    "# optimizer = optim.SGD(model.parameters(), lr=conf[\"learning_rate\"], momentum=0.9)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "completed_epochs = 0\n",
    "step = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# checkpoint = torch.load('../input/lastquerytransformer40ebundlefix/lqt-2021-07-25.pt')\n",
    "# completed_epochs = checkpoint['epoch']\n",
    "# step = checkpoint[\"step\"]\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# checkpoint[\"model_state_dict\"][\"emb.answer_emb.weight\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x_cat = batch['cat'].to(device)\n",
    "        x_cont = batch['cont'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        if \"lengths\" in batch:\n",
    "            seq_lengths = batch['lengths']\n",
    "        else:\n",
    "            seq_lengths = None\n",
    "        ypred = model(x_cat, x_cont, seq_lengths).squeeze(1)\n",
    "        loss = criterion(ypred, y)\n",
    "\n",
    "        losses.append(float(loss))\n",
    "        accuracies.append((torch.round(torch.sigmoid(ypred)) == y).float().mean().item())\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_score.extend(torch.sigmoid(ypred).cpu().numpy())\n",
    "\n",
    "    return mean(losses), mean(accuracies), roc_auc_score(y_true, y_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(evaluate(model, criterion, train_loader))\n",
    "print(evaluate(model, criterion, val_loader))\n",
    "# Time to run on laptop with 100k examples (train)\n",
    "# max_len=   0, sort_seq=True, shuffle=False, bs=16: 58 s\n",
    "# max_len=   0, sort_seq=False, shuffle=True, bs=16: 408 s\n",
    "# max_len=1728, sort_seq=True, shuffle=False, bs=16: 99 s\n",
    "# max_len=1728, sort_seq=True, shuffle=False, bs=32: 36 s\n",
    "# max_len=1728, sort_seq=True, shuffle=False, bs=64: 32 s\n",
    "# max_len=1728, sort_seq=True, shuffle=False, bs=128: 27 s\n",
    "# max_len=1728, sort_seq=False, shuffle=True, bs=128: 200 s"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.7211155409018198, 0.4557395833333333, 0.4390699219409951)\n",
      "(0.722421088279822, 0.4519934531969902, 0.4317338440650058)\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "log_dir = \"tensorboard\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "if log_dir is not None:\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "for e in range(completed_epochs, completed_epochs+conf['epochs']):\n",
    "    print(\"Epoch \", e)\n",
    "    for batch in train_loader:\n",
    "        #print(scaler.get_scale())\n",
    "        model.train()\n",
    "        x_cat = batch['cat'].to(device)\n",
    "        x_cont = batch['cont'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        if \"lengths\" in batch:\n",
    "            seq_lengths = batch['lengths']\n",
    "        else:\n",
    "            seq_lengths = None\n",
    "        with autocast(enabled=False):\n",
    "            ypred = model(x_cat, x_cont, seq_lengths).squeeze(1)\n",
    "            if ypred.isnan().sum() or ypred.isinf().sum():\n",
    "                print(\"Nan value in output!\")\n",
    "            loss = criterion(ypred, y)\n",
    "            if loss.isnan().sum() or loss.isinf().sum():\n",
    "                print(\"Nan value in loss!\")\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        #scaler.scale(loss).backward()\n",
    "        \n",
    "        # unscale to apply gradient clipping\n",
    "        #scaler.unscale_(optimizer)\n",
    "        if conf[\"clip_value\"]:\n",
    "            nn.utils.clip_grad_value_(model.parameters(), clip_value=conf[\"clip_value\"])\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        #scaler.step(optimizer)\n",
    "        #scaler.update()\n",
    "\n",
    "        step += 1\n",
    "        if step % conf['eval_steps'] == 0:\n",
    "            train_loss, train_acc, train_auc = evaluate(model, criterion, train_loader)\n",
    "            print(\"Step\", step, end=\"\\n\")\n",
    "            print(\"Train loss:\", \"{:.3f}\".format(train_loss), end=\" \")\n",
    "            print(\"Train accuracy:\", \"{:.3f}\".format(train_acc), end=' ')\n",
    "            print(\"Train AUC:\", \"{:.3f}\".format(train_auc), end='\\n')\n",
    "            if log_dir is not None:\n",
    "                writer.add_scalar(\"train/loss\", train_loss, step)\n",
    "                writer.add_scalar(\"train/accuracy\", train_acc, step)\n",
    "                writer.add_scalar(\"train/auc\", train_auc, step)\n",
    "            if val_loader is not None:\n",
    "                val_loss, val_acc, val_auc = evaluate(model, criterion, val_loader)\n",
    "                print(\"Valid loss:\", \"{:.3f}\".format(val_loss), end=\" \")\n",
    "                print(\"Valid accuracy:\", \"{:.3f}\".format(val_acc), end=\" \")\n",
    "                print(\"Valid AUC:\", \"{:.3f}\".format(val_auc), end='\\n')\n",
    "                if log_dir is not None:\n",
    "                    writer.add_scalar(\"eval/loss\", val_loss, step)\n",
    "                    writer.add_scalar(\"eval/acc\", val_acc, step)\n",
    "                    writer.add_scalar(\"eval/auc\", val_auc, step)\n",
    "                #scheduler.step(val_loss)\n",
    "\n",
    "if log_dir is not None:\n",
    "    writer.close()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0\n",
      "Step 250\n",
      "Train loss: 0.641 Train accuracy: 0.627 Train AUC: 0.680\n",
      "Valid loss: 0.645 Valid accuracy: 0.622 Valid AUC: 0.674\n",
      "Step 500\n",
      "Train loss: 0.643 Train accuracy: 0.625 Train AUC: 0.679\n",
      "Valid loss: 0.646 Valid accuracy: 0.622 Valid AUC: 0.675\n",
      "Step 750\n",
      "Train loss: 0.668 Train accuracy: 0.580 Train AUC: 0.669\n",
      "Valid loss: 0.670 Valid accuracy: 0.574 Valid AUC: 0.659\n",
      "Epoch  1\n",
      "Step 1000\n",
      "Train loss: 0.640 Train accuracy: 0.629 Train AUC: 0.686\n",
      "Valid loss: 0.646 Valid accuracy: 0.623 Valid AUC: 0.679\n",
      "Step 1250\n",
      "Train loss: 0.636 Train accuracy: 0.635 Train AUC: 0.688\n",
      "Valid loss: 0.642 Valid accuracy: 0.624 Valid AUC: 0.679\n",
      "Step 1500\n",
      "Train loss: 0.651 Train accuracy: 0.602 Train AUC: 0.682\n",
      "Valid loss: 0.658 Valid accuracy: 0.592 Valid AUC: 0.666\n",
      "Epoch  2\n",
      "Step 1750\n",
      "Train loss: 0.628 Train accuracy: 0.647 Train AUC: 0.703\n",
      "Valid loss: 0.638 Valid accuracy: 0.630 Valid AUC: 0.686\n",
      "Step 2000\n",
      "Train loss: 0.627 Train accuracy: 0.646 Train AUC: 0.702\n",
      "Valid loss: 0.639 Valid accuracy: 0.623 Valid AUC: 0.681\n",
      "Step 2250\n",
      "Train loss: 0.640 Train accuracy: 0.625 Train AUC: 0.700\n",
      "Valid loss: 0.653 Valid accuracy: 0.604 Valid AUC: 0.672\n",
      "Epoch  3\n",
      "Step 2500\n",
      "Train loss: 0.617 Train accuracy: 0.657 Train AUC: 0.719\n",
      "Valid loss: 0.633 Valid accuracy: 0.637 Valid AUC: 0.693\n",
      "Step 2750\n",
      "Train loss: 0.614 Train accuracy: 0.663 Train AUC: 0.721\n",
      "Valid loss: 0.633 Valid accuracy: 0.636 Valid AUC: 0.692\n",
      "Step 3000\n",
      "Train loss: 0.631 Train accuracy: 0.642 Train AUC: 0.701\n",
      "Valid loss: 0.650 Valid accuracy: 0.607 Valid AUC: 0.662\n",
      "Epoch  4\n",
      "Step 3250\n",
      "Train loss: 0.605 Train accuracy: 0.672 Train AUC: 0.733\n",
      "Valid loss: 0.630 Valid accuracy: 0.642 Valid AUC: 0.699\n",
      "Step 3500\n",
      "Train loss: 0.602 Train accuracy: 0.675 Train AUC: 0.737\n",
      "Valid loss: 0.631 Valid accuracy: 0.638 Valid AUC: 0.697\n",
      "Step 3750\n",
      "Train loss: 0.615 Train accuracy: 0.658 Train AUC: 0.730\n",
      "Valid loss: 0.646 Valid accuracy: 0.618 Valid AUC: 0.684\n",
      "Epoch  5\n",
      "Step 4000\n",
      "Train loss: 0.597 Train accuracy: 0.680 Train AUC: 0.744\n",
      "Valid loss: 0.633 Valid accuracy: 0.642 Valid AUC: 0.701\n",
      "Step 4250\n",
      "Train loss: 0.596 Train accuracy: 0.682 Train AUC: 0.745\n",
      "Valid loss: 0.632 Valid accuracy: 0.640 Valid AUC: 0.697\n",
      "Step 4500\n",
      "Train loss: 0.607 Train accuracy: 0.668 Train AUC: 0.740\n",
      "Valid loss: 0.648 Valid accuracy: 0.623 Valid AUC: 0.688\n",
      "Epoch  6\n",
      "Step 4750\n",
      "Train loss: 0.588 Train accuracy: 0.688 Train AUC: 0.755\n",
      "Valid loss: 0.633 Valid accuracy: 0.645 Valid AUC: 0.703\n",
      "Step 5000\n",
      "Train loss: 0.586 Train accuracy: 0.690 Train AUC: 0.756\n",
      "Valid loss: 0.633 Valid accuracy: 0.643 Valid AUC: 0.699\n",
      "Step 5250\n",
      "Train loss: 0.595 Train accuracy: 0.679 Train AUC: 0.752\n",
      "Valid loss: 0.645 Valid accuracy: 0.628 Valid AUC: 0.692\n",
      "Epoch  7\n",
      "Step 5500\n",
      "Train loss: 0.580 Train accuracy: 0.695 Train AUC: 0.763\n",
      "Valid loss: 0.636 Valid accuracy: 0.641 Valid AUC: 0.700\n",
      "Step 5750\n",
      "Train loss: 0.573 Train accuracy: 0.703 Train AUC: 0.769\n",
      "Valid loss: 0.635 Valid accuracy: 0.642 Valid AUC: 0.703\n",
      "Step 6000\n",
      "Train loss: 0.590 Train accuracy: 0.685 Train AUC: 0.761\n",
      "Valid loss: 0.654 Valid accuracy: 0.627 Valid AUC: 0.691\n",
      "Epoch  8\n",
      "Step 6250\n",
      "Train loss: 0.566 Train accuracy: 0.706 Train AUC: 0.776\n",
      "Valid loss: 0.639 Valid accuracy: 0.646 Valid AUC: 0.704\n",
      "Step 6500\n",
      "Train loss: 0.569 Train accuracy: 0.707 Train AUC: 0.775\n",
      "Valid loss: 0.645 Valid accuracy: 0.640 Valid AUC: 0.700\n",
      "Step 6750\n",
      "Train loss: 0.580 Train accuracy: 0.694 Train AUC: 0.771\n",
      "Valid loss: 0.657 Valid accuracy: 0.631 Valid AUC: 0.693\n",
      "Epoch  9\n",
      "Step 7000\n",
      "Train loss: 0.558 Train accuracy: 0.713 Train AUC: 0.784\n",
      "Valid loss: 0.645 Valid accuracy: 0.648 Valid AUC: 0.706\n",
      "Step 7250\n",
      "Train loss: 0.561 Train accuracy: 0.714 Train AUC: 0.783\n",
      "Valid loss: 0.657 Valid accuracy: 0.639 Valid AUC: 0.697\n",
      "Step 7500\n",
      "Train loss: 0.571 Train accuracy: 0.702 Train AUC: 0.781\n",
      "Valid loss: 0.661 Valid accuracy: 0.634 Valid AUC: 0.693\n",
      "Epoch  10\n",
      "Step 7750\n",
      "Train loss: 0.549 Train accuracy: 0.719 Train AUC: 0.792\n",
      "Valid loss: 0.657 Valid accuracy: 0.644 Valid AUC: 0.701\n",
      "Step 8000\n",
      "Train loss: 0.554 Train accuracy: 0.720 Train AUC: 0.789\n",
      "Valid loss: 0.666 Valid accuracy: 0.638 Valid AUC: 0.691\n",
      "Step 8250\n",
      "Train loss: 0.562 Train accuracy: 0.710 Train AUC: 0.788\n",
      "Valid loss: 0.677 Valid accuracy: 0.636 Valid AUC: 0.692\n",
      "Epoch  11\n",
      "Step 8500\n",
      "Train loss: 0.541 Train accuracy: 0.727 Train AUC: 0.800\n",
      "Valid loss: 0.671 Valid accuracy: 0.644 Valid AUC: 0.701\n",
      "Step 8750\n",
      "Train loss: 0.540 Train accuracy: 0.730 Train AUC: 0.801\n",
      "Valid loss: 0.673 Valid accuracy: 0.637 Valid AUC: 0.692\n",
      "Step 9000\n",
      "Train loss: 0.550 Train accuracy: 0.719 Train AUC: 0.799\n",
      "Valid loss: 0.688 Valid accuracy: 0.636 Valid AUC: 0.690\n",
      "Epoch  12\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "torch.save({\n",
    "            'epoch': e,\n",
    "            \"step\": step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss': loss,\n",
    "            }, \"lqt-\"+str(date.today())+\".pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Error analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# train_dataset[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[4887,    5,    3],\n",
       "        [4033,    5,    1],\n",
       "        [5928,    5,    1],\n",
       "        ...,\n",
       "        [9820,    5,    2],\n",
       "        [9507,    5,    1],\n",
       "        [8874,    5,    2]]),\n",
       " array([[249000.,      0.],\n",
       "        [ 18000.,  21186.],\n",
       "        [ 31000.,  33844.],\n",
       "        ...,\n",
       "        [ 44000.,  78052.],\n",
       "        [ 17000.,  68609.],\n",
       "        [     0., 213212.]]),\n",
       " 1,\n",
       " 17609)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# batch = next(iter(train_loader)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# x_cat = batch['cat'].to(device)\n",
    "# x_cont = batch['cont'].to(device)\n",
    "# y = batch['y'].to(device)\n",
    "# seq_lengths = batch['lengths']#.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "# x_cat_copy = x_cat.clone()\n",
    "# x_cont_copy = x_cont.clone()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "# x_cont_copy[0, -5:, 1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2.2320e+05, 5.0000e+01, 5.0000e+01, 5.0000e+01, 5.0000e+01],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "# x_cont_copy[0, -5, 1] = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "# model(x_cat_copy, x_cont_copy, seq_lengths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.8957]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}