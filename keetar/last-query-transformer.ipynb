{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add utility_scripts in the current path so that they can be imported directly just like in interactive mode\n",
    "sys.path.append(os.path.abspath(\"../usr/lib/\"))\n",
    "for script_folder in os.listdir(\"../usr/lib/\"):\n",
    "    sys.path.append(os.path.abspath(\"../usr/lib/\"+script_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from statistics import mean, median\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lastquerytransformer import Riiid\n",
    "from riiidutils import RiiidDataset, riiid_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = os.environ.get('KAGGLE_KERNEL_RUN_TYPE','Localhost')\n",
    "if loc == 'Interactive' or loc == 'Localhost':\n",
    "    conf = {\n",
    "        'batch_size': 8,\n",
    "        'train_size': 40_000,\n",
    "        'epochs': 10,\n",
    "        'eval_steps': 1000\n",
    "    }\n",
    "# When it is run after an api push.\n",
    "elif loc == 'Batch':\n",
    "    conf = {\n",
    "        'batch_size': 3,\n",
    "        'train_size': 400_000,\n",
    "        'epochs': 30,\n",
    "        'eval_steps': 2500\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "# Introduction\n",
    "\n",
    "Le fichier train.csv comprend un peu plus de 100 millions de lignes.  \n",
    "Il y a un peu moins de 400 mille user_id uniques.  \n",
    "Le modèle utilisé dans ce notebook prend en entrée une série par utilisateur."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Features utilisées\n",
    "Pour chaque question de la série d'apprentissage d'un utilisateur quelconque  \n",
    "\n",
    "1. Question ID: correspond à content_id (lorsque l'élément est une question).\n",
    "2. Question part: correspond à part dans question.csv pour l'élément dont question_id correspondant à content_id\n",
    "3. Answer correctness: valeur de answered_correctly de l'exemple (ou target encoding de la question ?)\n",
    "4. Current question elapsed time: prior question de la question suivante.\n",
    "5. Timestamp difference: current question timestamp - timestamp of the last question from the same user"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Chargement des données et instanciation des datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/riiid-sequences/users_y.pickle', 'rb') as f:\n",
    "    users_y = pickle.load(f)\n",
    "with open('../input/riiid-sequences/users_cat.pickle', 'rb') as f:\n",
    "    users_cat = pickle.load(f)\n",
    "with open('../input/riiid-sequences/users_cont.pickle', 'rb') as f:\n",
    "    users_cont = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12\n",
    "cat_train, cat_val, cont_train, cont_val, y_train, y_val = train_test_split(users_cat, users_cont, users_y, test_size=.05, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = cat_train[:conf['train_size']]\n",
    "cont_train = cont_train[:conf['train_size']]\n",
    "y_train = y_train[:conf['train_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train examples: 40000\nNumber of valid examples: 19683\nTrain set answered_correctly average value: 0.469\nValid set answered_correctly average value: 0.475\nTrain set median sequence length: 40\nValid set median sequence length: 41\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train examples:\", len(y_train))\n",
    "print(\"Number of valid examples:\", len(y_val))\n",
    "print(\"Train set answered_correctly average value:\", \"{:.3}\".format(y_train.mean()))\n",
    "print(\"Valid set answered_correctly average value:\", \"{:.3}\".format(y_val.mean()))\n",
    "print(\"Train set median sequence length:\", \"{:.0f}\".format(median([user_seq.shape[0] for user_seq in cat_train])))\n",
    "print(\"Valid set median sequence length:\", \"{:.0f}\".format(median([user_seq.shape[0] for user_seq in cat_val])))"
   ]
  },
  {
   "source": [
    "## Batches\n",
    "Dynamic Padding: ajout de padding batch par batch pour avoir une même longueur de séquence dans chaque batch.  \n",
    "Uniform size batching: on trie les utilisateurs par longueur de séquence, afin d'avoir des longueurs plus proches dans chaque batch  \n",
    "\n",
    "Afin de mettre en œuvre ces deux stratégies on va par simplicité trier au préalable et conjointement les listes batch_cat, batch_cont et batch_y par longueur des séquences dans batch_cat/batch_cont (c'est RiiidDataset qui s'en charge). Le DataLoader utilisera une fonction collate_fn permettant d'ajouter du padding dynamiquement batch par batch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RiiidDataset(cat_train, cont_train, y_train)\n",
    "val_dataset = RiiidDataset(cat_val, cont_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=conf['batch_size'], shuffle=False, collate_fn=riiid_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4*conf['batch_size'], shuffle=False, collate_fn=riiid_collate_fn)"
   ]
  },
  {
   "source": [
    "# Modèle\n",
    "Pour le modèle, l'auteur s'est inspiré de la solution arrivée 3è à la compétition data Science bowl 2019 ([discussion](https://www.kaggle.com/c/data-science-bowl-2019/discussion/127891), [code](https://www.kaggle.com/limerobot/dsb2019-v77-tr-dt-aug0-5-3tta))  \n",
    "La procédure de création des embeddings est expliquée, on suppose que l'auteur s'est basé dessus.\n",
    "## Embeddings\n",
    "On utilise un embedding catégoriel pour les 3 premières variables et un embedding continu pour les 2 dernières.\n",
    "Contrairement à la solution du lien ci-dessus on utilise un embedding catégoriel par variable catégorielle plutôt qu'un embedding commun, comme recommandé [ici](https://discuss.pytorch.org/t/categorical-embeddings-can-i-only-have-1-categorical-column-per-embedding-layer/104681/3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximums = {'question_id': 13523, 'part': 7, 'answered_correctly': 3}\n",
    "model = Riiid(maximums, dropout=0.2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=4e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "completed_epochs = 0"
   ]
  },
  {
   "source": [
    "## Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('../input/lastquerytransformer40ebundlefix/lqt-2021-04-18.pt')\n",
    "# completed_epochs = checkpoint['epoch']\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            x_cat = batch['cat'].to(device)\n",
    "            x_cont = batch['cont'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "\n",
    "            ypred = model(x_cat, x_cont).squeeze(1)\n",
    "            loss = criterion(ypred, y)\n",
    "\n",
    "            losses.append(float(loss))\n",
    "            accuracies.append((torch.round(torch.sigmoid(ypred)) == y).float().mean().item())\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_score.extend(torch.sigmoid(ypred).cpu().numpy())\n",
    "\n",
    "    return mean(losses), mean(accuracies), roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.6967743617296219, 0.46935, 0.4751343083495947)\n",
      "(0.6961498752042845, 0.475023674258551, 0.4819299649195933)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, criterion, train_loader))\n",
    "print(evaluate(model, criterion, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_dir is not None:\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "step = 0\n",
    "for e in range(completed_epochs, completed_epochs+conf['epochs']):\n",
    "    print(\"Epoch \", e)\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        x_cat = batch['cat'].to(device)\n",
    "        x_cont = batch['cont'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "\n",
    "        ypred = model(x_cat, x_cont).squeeze(1)\n",
    "        loss = criterion(ypred, y)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        if step % conf['eval_steps'] == 0:\n",
    "            train_loss, train_acc, train_auc = evaluate(model, criterion, train_loader)\n",
    "            print(\"Step\", step, end=\"\\n\")\n",
    "            print(\"Train loss:\", \"{:.3f}\".format(train_loss), end=\" \")\n",
    "            print(\"Train accuracy:\", \"{:.3f}\".format(train_acc), end=' ')\n",
    "            print(\"Train AUC:\", \"{:.3f}\".format(train_auc), end='\\n')\n",
    "            if log_dir is not None:\n",
    "                writer.add_scalar(\"train/loss\", train_loss, step)\n",
    "                writer.add_scalar(\"train/accuracy\", train_acc, step)\n",
    "                writer.add_scalar(\"train/auc\", train_auc, step)\n",
    "            if val_loader is not None:\n",
    "                val_loss, val_acc, val_auc = evaluate(model, criterion, val_loader)\n",
    "                print(\"Valid loss:\", \"{:.3f}\".format(val_loss), end=\" \")\n",
    "                print(\"Valid accuracy:\", \"{:.3f}\".format(val_acc), end=\" \")\n",
    "                print(\"Valid AUC:\", \"{:.3f}\".format(val_auc), end='\\n')\n",
    "                if log_dir is not None:\n",
    "                    writer.add_scalar(\"eval/loss\", val_loss, step)\n",
    "                    writer.add_scalar(\"eval/acc\", val_acc, step)\n",
    "                    writer.add_scalar(\"eval/auc\", val_auc, step)\n",
    "\n",
    "if log_dir is not None:\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss': loss,\n",
    "            }, \"lqt-\"+str(date.today())+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}